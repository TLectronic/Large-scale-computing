# 热传导方程简介

二维热传导方程表示为：

$\frac{\partial u}{\partial t} = a(\frac{\partial^2u}{\partial x^2}+\frac{\partial^2u}{\partial y^2})$

其中：

- $u$是温度场
- $a$是扩散常数
- $t$是时间
- $x$ 和 $y$ 是空间坐标

为了数值求解该方程，使用有限差分法，将时间和空间离散化。把连续的偏导数替换为离散差分。

空间离散化：

考虑一个二维网格，其中 $u_{i,j}$ 表示网格点 $(i,j)$ 的温度。

使用中心差分法近似二阶导数：
$\frac{\partial^2u}{\partial x^2}\approx\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{(\Delta x)^2}$

$\frac{\partial^2u}{\partial y^2}\approx\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{(\Delta y)^2}$

其中$\Delta x$和$\Delta y$分别是$x$和$y$方向上的网格间距。

时间离散化：

对于时间导数，使用前向差分

$\frac{\partial u}{\partial t} = \frac{u^{n+1}_{i,j} - u^{n}_{i,j} }{\Delta t}$

其中$\Delta t$是时间步长，$u^n_{i+j}$和$u^{n+1}_{i,j}$分别是当前和下一时间步的温度。

把以上两个离散差分替换原来的连续偏导数，整理得以下方程：

$u^{n+1}_{i,j}=u^n_{i,j}+a\Delta t(\frac{u^n_{i+1,j}-2u^n_{i,j}+u^n_{i-1,j}}{(\Delta x)^2}+\frac{u^n_{i,j+1}-2u_{i,j}^n+u^n_{i,j-1}}{(\Delta y)^2})$

这也对应了代码中的场演化函数的计算。



时间步长$dt$的确定在数值方法中非常重要，关系到计算的稳定性和精度。对于显式方法（如前向差分法）选择合适的时间步长尤为关键、为了确保计算的稳定性，需要遵循某些稳定性条件。在热传导方程中，这被称为Courant-Friedrichs-Lewy（CFL）条件。

对于二维热传导方程：
$\frac{\partial u}{\partial t} = a(\frac{\partial^2u}{\partial x^2}+\frac{\partial^2u}{\partial y^2})$

使用显式方法时，CFL条件要求时间步长$dt$必须足够小，以防止数值解发散，这一条件可以表达为：

$dt\leq\frac{1}{2}\frac{dx^2dy^2}{a(dx^2+dy^2)}$

代码中直接令 $dt=\frac{1}{2}\frac{dx^2dy^2}{a(dx^2+dy^2)}$



如果考虑的介质不是整个空间，则为了得到方程的唯一解，必须指定u的边界条件。如果介质是整个空间，为了得到唯一性，必须假定解的增长速度有个指数型的上界，此假定吻合实验结果。

# Mpi4py

MPI（Message Passing Interface）是基于消息传递的并行计算框架。

使用Mpi4py库进行并行计算，主要思想是：将原始温度场按行划分给各个进程，每个进程负责处理一个连续的行区间。

场演化函数evolve与Sequential的代码完全相同

比Sequential代码增加了一个交换边界数据的函数exchange

# Numba

Numba是一个用于加速Python数值计算的库，它通过将Python代码编译为机器码来提高计算速度。

场演化函数和Sequential的代码出现了差异，变成了两层循环结构。

原因：Numba主要针对单个CPU核心的优化，通过循环结构可以充分利用CPU的计算能力。

# Dask



Dask将场演化函数evolve标记为延迟计算函数。即函数调用并不立即执行，而是创建了一个延迟执行的计算图。在实际需要获取结果时，Dask根据计算图自动调度和执行计算，从而有效利用计算资源。

场演化函数和Sequential的代码又有不同，这次场演化函数不需要传递上一步的场状态这个参数。

原因：Dask版本的evolve被dask.delayed装饰器修饰，意味着这是一个延迟计算函数，函数调用并不会立即执行，而是构建了一个延迟执行的任务图。Dask会根据任务图的依赖关系自动追踪和执行计算，也就是说可以自动追踪和传递上一步场的状态，因此不需要上一步场状态这个参数。

# PyCUDA

PyCUDA通过将计算任务分配给GPU上的多个线程块和线程，利用GPU的并行计算能力来加速计算。具体实现方法就是将计算任务转换为CUDA内核函数，然后在GPU上并行执行这些CUDA内核函数。

这个版本就直接把场演化函数写道CUDA内核中了，逻辑基本不变。

# PySpark

PySpark使用RDD来并行处理数据，将温度场分割成多个部分，并作为RDD的分区进行处理。每个分区都被分配给集群中的一个计算节点进行处理，这样可以利用集群中多个计算节点的计算资源，并行处理数据，加快计算速度。